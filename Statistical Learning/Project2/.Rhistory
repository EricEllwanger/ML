s.window=3,
method='ets',
ic='bic',
opt.crit='mae')
}else if(method == 'arima'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='arima',
ic='bic')
}
raw.pred[, j] <- fc$mean
}
for(j in 1:ncol(tr)){
o <- order(crl[j, ], decreasing=TRUE)
score <- sort(crl[j, ], decreasing=TRUE)
if(length(o[score >= level1]) > k){
top.idx <- o[score >= level1]
}else{
top.idx <- o[score >= level2]
top.idx <- top.idx[1:min(length(top.idx),k)]
}
top <- raw.pred[, top.idx]
if (length(top.idx) > 1){
pred <- rowMeans(top)
}else{
pred <- as.numeric(top)
}
pred <- pred * attr(tr.scale, 'scaled:scale')[j]
pred <- pred + attr(tr.scale, 'scaled:center')[j]
test[, j + 1] <- pred
}
test
}
fourier.arima <- function(train, test, k){
# This model is a regression on k sin/cos pairs of Fourier series terms
# with non-seasonal arima errors. The call to auto.arima() crashes on data
# with too many missing values, or too many identical values, so this
# function falls back to another, more stable method in that case.
#
# args:
# train - A matrix of Weekly_Sales values from the training set of dimension
#         (number of weeeks in training data) x (number of stores)
# test - An all-zeros matrix of dimension:
#       (number of weeeks in training data) x (number of stores)
#       The forecasts are written in place of the zeros.
# k - number of sin/cos pair to use
#
# returns:
#  the test(forecast) data frame with the forecasts filled in
horizon <- nrow(test)
for(j in 2:ncol(train)){
if(sum(is.na(train[, j])) > nrow(train)/3){
test[, j] <- fallback(train[,j], horizon)
print(paste('Fallback on store:', names(train)[j]))
}else{
# fit arima model
s <- ts(train[, j], frequency=365/7)
model <- auto.arima(s, xreg=fourier(s, k), ic='bic', seasonal=FALSE)
fc <- forecast(model, h=horizon, xreg=fourierf(s, k, horizon))
test[, j] <- as.numeric(fc$mean)
}
}
test
}
seasonal.arima.svd <- function(train, test, n.comp){
# Replaces the training data with a rank-reduced approximation of itself
# and then produces seasonal arima forecasts for each store.
#
# args:
# train - A matrix of Weekly_Sales values from the training set of dimension
#         (number of weeeks in training data) x (number of stores)
# test - An all-zeros matrix of dimension:
#       (number of weeeks in training data) x (number of stores)
#       The forecasts are written in place of the zeros.
# n.comp - the number of components to keep in the singular value
#         decomposition that is performed for preprocessing
#
# returns:
#  the test(forecast) data frame with the forecasts filled in
horizon <- nrow(test)
tr <- preprocess.svd(train, n.comp)
for(j in 2:ncol(tr)){
if(sum(is.na(train[, j])) > nrow(train)/3){
# Use DE model as fallback
test[, j] <- fallback(tr[,j], horizon)
store.num <- names(train)[j]
print(paste('Fallback on store:', store.num))
}else{
# fit arima model
s <- ts(tr[, j], frequency=52)
model <- auto.arima(s, ic='bic', seasonal.test='ch')
fc <- forecast(model, h=horizon)
test[, j] <- as.numeric(fc$mean)
}
}
test
}
fallback <- function(train, horizon){
# This method is a fallback forecasting method in the case that there are
# enough NA's to possibly crash arima models. It takes one seasonal
# difference, forecasts with a level-only exponential model, and then
# inverts the seasonal difference.
#
# args:
# train - a vector of training data for one store
# horizon - the forecast horizon in weeks
#
# returns:
#  a vector of forecast values
s <- ts(train, frequency=52)
s[is.na(s)] <- 0
fc <- ses(diff(s, 52), h=horizon)
result <- diffinv(fc$mean, lag=52, xi=s[length(s) - 51:0])
result[length(result) - horizon:1 + 1]
}
preprocess.svd <- function(train, n.comp){
# Replaces the training data with a rank-reduced approximation of itself.
# This is for noise reduction. The intuition is that characteristics
# that are common across stores (within the same department) are probably
# signal, while those that are unique to one store may be noise.
#
# args:
# train - A matrix of Weekly_Sales values from the training set of dimension
#         (number of weeeks in training data) x (number of stores)
# n.comp - the number of components to keep in the singular value
#         decomposition
#
# returns:
#  the rank-reduced approximation of the training data
train[is.na(train)] <- 0
z <- svd(train[, 2:ncol(train)], nu=n.comp, nv=n.comp)
s <- diag(z$d[1:n.comp])
train[, 2:ncol(train)] <- z$u %*% s %*% t(z$v)
train
}
stlf.svd <- function(train, test, model.type, n.comp){
# Replaces the training data with a rank-reduced approximation of itself,
# then forecasts each store using stlf() from the forecast package.
# That function performs an STL decomposition on each series, seasonally
# adjusts the data, non-seasonally forecasts the seasonally adjusted data,
# and then adds in the naively extended seasonal component to get the
# final forecast.
#
# args:
# train - A matrix of Weekly_Sales values from the training set of dimension
#         (number of weeeks in training data) x (number of stores)
# test - An all-zeros matrix of dimension:
#       (number of weeeks in training data) x (number of stores)
#       The forecasts are written in place of the zeros.
# model.type - one of 'ets' or 'arima', specifies which type of model to
#        use for the non-seasonal forecast
# n.comp - the number of components to keep in the singular value
#         decomposition that is performed for preprocessing
#
# returns:
#  the test(forecast) data frame with the forecasts filled in
horizon <- nrow(test)
train <- preprocess.svd(train, n.comp)
for(j in 2:ncol(train)){
s <- ts(train[, j], frequency=52)
if(model.type == 'ets'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='ets',
ic='bic',
opt.crit='mae')
}else if(model.type == 'arima'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='arima',
ic='bic')
}else{
stop('Model type must be one of ets or arima.')
}
pred <- as.numeric(fc$mean)
test[, j] <- pred
}
test
}
glm
install.packages('forecast')
library(forecast)
mypredict = function(){
model.type == 'ets'
start_date <- ymd("2011-03-01") %m+% months(2 * (t - 1))
end_date <- ymd("2011-05-01") %m+% months(2 * (t - 1))
test_current <- test %>%
filter(Date >= start_date & Date < end_date) %>%
select(-IsHoliday)
if (t>1){
train <<- train %>% add_row(new_train)
}
# not all depts need prediction
test_depts <- unique(test_current$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test_current %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
for(store in test_stores){
tmp_train <- train_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
tmp_test <- test_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
tmp_train$Wk = factor(tmp_train$Wk, levels = 1:52)
tmp_test$Wk = factor(tmp_test$Wk, levels = 1:52)
horizon <- nrow(tmp_test)
#        train <- preprocess.svd(train, n.comp)
for(j in 2:ncol(tmp_train)){
s <- ts(tmp_train[, j], frequency=52)
if(model.type == 'ets'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='ets',
ic='bic',
opt.crit='mae')
}
else if(model.type == 'arima'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='arima',
ic='bic')
}
else{
stop('Model type must be one of ets or arima.')
}
pred <- as.numeric(fc$mean)
test[, j] <- pred
}
test
}
tmp_test <- tmp_test %>%
mutate(Weekly_Pred = tmp_pred[,1]) %>%
select(-Wk, -Yr)
test_pred <- test_pred %>% bind_rows(tmp_test)
}
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file,
col_types = cols())
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
mypredict = function(){
model.type = 'ets'
start_date <- ymd("2011-03-01") %m+% months(2 * (t - 1))
end_date <- ymd("2011-05-01") %m+% months(2 * (t - 1))
test_current <- test %>%
filter(Date >= start_date & Date < end_date) %>%
select(-IsHoliday)
if (t>1){
train <<- train %>% add_row(new_train)
}
# not all depts need prediction
test_depts <- unique(test_current$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test_current %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
for(store in test_stores){
tmp_train <- train_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
tmp_test <- test_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
tmp_train$Wk = factor(tmp_train$Wk, levels = 1:52)
tmp_test$Wk = factor(tmp_test$Wk, levels = 1:52)
horizon <- nrow(tmp_test)
#        train <- preprocess.svd(train, n.comp)
for(j in 2:ncol(tmp_train)){
s <- ts(tmp_train[, j], frequency=52)
if(model.type == 'ets'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='ets',
ic='bic',
opt.crit='mae')
}
else if(model.type == 'arima'){
fc <- stlf(s,
h=horizon,
s.window=3,
method='arima',
ic='bic')
}
else{
stop('Model type must be one of ets or arima.')
}
pred <- as.numeric(fc$mean)
test[, j] <- pred
}
test
}
tmp_test <- tmp_test %>%
mutate(Weekly_Pred = tmp_pred[,1]) %>%
select(-Wk, -Yr)
test_pred <- test_pred %>% bind_rows(tmp_test)
}
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# save weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
# load fold file
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file,
col_types = cols())
# extract predictions matching up to the current fold
scoring_tbl <- new_train %>%
left_join(test_pred, by = c('Date', 'Store', 'Dept'))
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
ggseasonplot(a10, year.labels=TRUE, year.labels.left=TRUE) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
ggseasonplot(train, year.labels=TRUE, year.labels.left=TRUE) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
y <- ts(train, start=2010)
ggseasonplot(train, year.labels=TRUE, year.labels.left=TRUE) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
y <- ts(train, start=2010)
ggseasonplot(y, year.labels=TRUE, year.labels.left=TRUE) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
y <- ts(train, start=2010)
ggseasonplot(y) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
y <- ts(train)
ggseasonplot(y) +
ylab("$ million") +
ggtitle("Seasonal plot: antidiabetic drug sales")
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
print(train)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
test_depts <- unique(test_current$Dept)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
test_depts <- unique(train$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test_current %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
test_depts <- unique(train$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
}
tmp_train <- train_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
train <- readr::read_csv('train_ini.csv')
setwd("C:\\Users\\frell\\Documents\\GitHub\\PSL\\Project2")
train <- readr::read_csv('train_ini.csv')
train
head(train)
train[0:10,]
train[0:100,]
train[0:100,]
train[0:5,]
train %>% filter(Dept == 1)
train <- readr::read_csv('train_ini.csv')
train_dept_data = train %>% filter(Dept == 1)
train_dept_data
test_depts = unique(train$Dept)
test_depts
train %>% filter(Dept == 2)
a = train %>% filter(Dept == 2)
length(a)
nrow(a)
a.isna
a[is.na]
is.na(a)
is.na(a) == TRUE
a[is.na(a) == TRUE,]
a[is.na(a) == TRUE]
a[,is.na(a) == TRUE]
a[is.na(a) == TRUE]
is.na(a) == TRUE
sum(is.na(a) == TRUE)
a
library(lubridate)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
test_depts <- unique(train$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
tmp_train <- train_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
print(tmp_train)
}
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
test_depts <- unique(train$Dept)
test_pred <- NULL
for(dept in test_depts){
train_dept_data <- train %>% filter(Dept == dept)
test_dept_data <- test %>% filter(Dept == dept)
# no need to consider stores that do not need prediction
# or do not have training samples
train_stores <- unique(train_dept_data$Store)
test_stores <- unique(test_dept_data$Store)
test_stores <- intersect(train_stores, test_stores)
for (store in test_stores) {
tmp_train <- train_dept_data %>%
filter(Store == store) %>%
mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
mutate(Yr = year(Date))
print(tmp_train)
}
}
